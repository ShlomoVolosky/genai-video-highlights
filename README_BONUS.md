# ğŸ§  Bonus Task â€” Neural Network Tic-Tac-Toe Player (TensorFlow Edition)

## ğŸ¯ Overview
This bonus module implements a **self-learning Tic-Tac-Toe AI** using **TensorFlow 2 (Keras)**.  
The agent learns from scratch with the **REINFORCE policy-gradient algorithm** and can later play against a human using a simple GUI.

| Difficulty | Opponent Type | Description |
|-------------|---------------|--------------|
| ğŸŸ¢ **Easy** | Random moves | Opponent plays randomly |
| ğŸŸ¡ **Medium** | Heuristic rules | Opponent uses fixed smart logic (center, corners, block, win) |
| ğŸ”´ **Hard** | Self-play | The agent competes against itself to improve |

---

## ğŸ§© Folder Structure
```
bonus/
â””â”€â”€ ttt/
    â”œâ”€â”€ env.py           # Tic-Tac-Toe environment (rules, rewards)
    â”œâ”€â”€ model.py         # TensorFlow/Keras policy network
    â”œâ”€â”€ opponents.py     # Random and heuristic opponents
    â”œâ”€â”€ train.py         # REINFORCE training loop + plots
    â”œâ”€â”€ game.py          # Tkinter GUI (human vs AI)
    â””â”€â”€ __init__.py
```

Everything runs **on CPU** â€” no GPU or CUDA required.

---

## âš™ï¸ Setup

### 1ï¸âƒ£ Create a virtual environment
```bash
python3 -m venv .venv-ttt
source .venv-ttt/bin/activate
# Windows: .venv-ttt\Scripts\activate
```

### 2ï¸âƒ£ Install dependencies
Create `requirements-bonus-ttt-tf.txt`:
```txt
tensorflow-cpu==2.16.1
matplotlib==3.9.0
numpy==1.26.4
```

Then install:
```bash
pip install -r requirements-bonus-ttt-tf.txt
```

If the GUI fails due to missing Tkinter on Linux:
```bash
sudo apt update && sudo apt install -y python3-tk
```

---

## ğŸ§  How It Works

### ğŸ§± Environment (`env.py`)
- 3Ã—3 board flattened into a 9-vector (`1 = X`, `-1 = O`, `0 = empty`).
- Detects wins, draws, and illegal moves.
- Reward system:
  - `+1` â†’ AI win
  - `âˆ’1` â†’ AI loss
  - `0` â†’ Draw or ongoing

### ğŸ§® Model (`model.py`)
A compact **policy network**:
```
Input (9) â†’ Dense(64, tanh) â†’ Dense(64, tanh) â†’ Dense(9, linear)
```
Outputs **logits** for all moves, converted to probabilities via a **masked softmax** so illegal moves are ignored.

### ğŸ¯ Training (`train.py`)
Implements the **REINFORCE** policy gradient algorithm:

âˆ‡J(Î¸) = E[Î£ âˆ‡Î¸ log Ï€Î¸(a_t|s_t) G_t]

Training steps:
1. Play games and record `(state, action, reward)` each turn.
2. Compute discounted returns G_t.
3. Recompute logits under one GradientTape.
4. Apply gradient ascent on âˆ’Î£(log Ï€ Ã— G).

Outputs:
```
bonus_out_tf/
â”œâ”€â”€ ttt_policy_<difficulty>.keras
â””â”€â”€ training_<difficulty>.png
```

### ğŸ¨ Visualization
- **Left plot:** Policy loss across episodes  
- **Right plot:** Rolling win/draw/loss rates (smoothed)

---

## ğŸ•¹ï¸ Play the Game (`game.py`)

Run the GUI:
```bash
python bonus/ttt/game.py
```

Interface:
- 3Ã—3 grid buttons
- Dropdown for difficulty (`easy`, `medium`, `hard`)
- Buttons:
  - **Load Model** â†’ load trained model from `bonus_out_tf/ttt_policy_<difficulty>.keras`
  - **Reset** â†’ start a new game

Gameplay:
- AI = **X**  
- Human = **O**  
- Messages: â€œAI (X) wins!â€, â€œDraw.â€, or â€œYou (O) win!â€

---

## ğŸ‹ï¸ Training Commands

| Mode   | Command                                                          | Description     |
|--------|------------------------------------------------------------------|-----------------|
| Easy   | `python bonus/ttt/train.py --opponent easy --episodes 4000`      | vs random       |
| Medium | `python bonus/ttt/train.py --opponent medium --episodes 6000`    | vs heuristic    |
| Hard   | `python bonus/ttt/train.py --opponent hard --episodes 12000`     | self-play       |

Optional flags:
```
--episodes <int>   # number of games
--gamma <float>    # discount factor (default 0.99)
--lr <float>       # learning rate (default 0.002)
--log-every <int>  # log frequency (default 200)
--outdir <path>    # output folder (default bonus_out_tf)
```

Example:
```bash
python bonus/ttt/train.py --opponent medium --episodes 8000 --lr 0.001
```

---

## ğŸ“Š Sample Output
```
Training vs: easy
[  200] loss=0.4829  win/draw/lose (last 200): 68.5/24.5/7.0%
[  400] loss=0.3124  win/draw/lose (last 200): 82.0/12.0/6.0%
Saved: bonus_out_tf/ttt_policy_easy.keras
Saved plot: bonus_out_tf/training_easy.png
```

Then:
```bash
python bonus/ttt/game.py
```

---

## ğŸ§¾ Notes & Tips
- **CPU-only**, no GPU required.  
- If Tkinter is missing on Linux: `sudo apt install python3-tk`.  
- Reproducibility:
  ```python
  import numpy as np, tensorflow as tf
  np.random.seed(0)
  tf.random.set_seed(0)
  ```

---

## ğŸ§  Key Learning Concepts

| Concept | Explanation |
|--------|-------------|
| **Policy Gradient (REINFORCE)** | Learns a probability distribution over actions instead of fixed rules |
| **Self-Play** | Agent improves by training against itself |
| **Masked Softmax** | Ensures only legal moves are chosen |
| **Reward Shaping** | +1 (win), âˆ’1 (loss), 0 (draw) |
| **Visualization** | Makes policy improvement interpretable |

---

## âœ… Summary

| Component | Description |
|-----------|-------------|
| Framework | TensorFlow 2 (Keras API) |
| Algorithm | Policy Gradient (REINFORCE) |
| Environment | Custom Tic-Tac-Toe |
| Opponents | Random / Heuristic / Self-play |
| Output | `.keras` models + training plots |
| Interface | Tkinter GUI |
| Hardware | CPU-only |

---

## ğŸš€ Quickstart
```bash
# Setup
python3 -m venv .venv-ttt && source .venv-ttt/bin/activate
pip install -r requirements-bonus-ttt-tf.txt

# Train (choose difficulty)
python bonus/ttt/train.py --opponent easy --episodes 4000

# Play
python bonus/ttt/game.py
```

---

## ğŸ End Result
After training:
- Youâ€™ll have a TensorFlow policy that can **intelligently play Tic-Tac-Toe**.  
- You can **visualize its learning curve**.  
- You can **challenge it in real time** via the GUI.
